# Implementing GANs: DCGAN, WGAN, and WGAN-GP

## Table of Contents
1. [The Original GAN Framework](#intro)
2. [DCGAN: Deep Convolutional GAN](#dcgan)
3. [WGAN: Wasserstein GAN](#wgan)
4. [WGAN-GP: WGAN with Gradient Penalty](#wgangp)
5. [Summary, Key Intuitions and Implementation](#summary) 

Generative Adversarial Networks (GANs) have transformed the field of generative modeling by pitting two networks—the generator
and the discriminator—against each other in a minimax game. In this post, we explore the mathematical foundations and 
intuitions behind GANs, and then dive into three influential architectures: DCGAN, WGAN, and WGAN-GP. I'll be also sharing 
fully working implementaiton of all of them so play with them if you find the topic interesting.

---

## 1. The Original GAN Framework<a name="intro"></a>

### 1.1. Basic Setup and Objective

In the classic GAN formulation introduced by Goodfellow et al. (2014), two networks are trained simultaneously:

- **Generator $G$:** Maps a noise vector $z \sim p_z$ (often Gaussian or uniform) into the data space.
- **Discriminator $D$:** Tries to distinguish real data $x \sim p_{\text{data}}$ from fake data $G(z)$.

The original minimax objective is defined as:

$$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}} \big[\log D(x)\big] + \mathbb{E}_{z \sim p_z} \big[\log (1 - D(G(z)))\big].
$$

The discriminator is trained to maximize the probability of correctly classifying real versus generated data, while the generator aims to fool the discriminator.

### 1.2. Mathematical Intuition

When $D$ is optimal, it can be shown that

$$
D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)},
$$

and the minimax game minimizes the Jensen–Shannon divergence between $p_{\text{data}}$ and the generator’s distribution $p_g$. However, due to issues like vanishing gradients and mode collapse, several variants have been proposed—most notably DCGAN, WGAN, and WGAN-GP.

---

## 2. DCGAN: Deep Convolutional GAN<a name="dcgan"></a>

### 2.1. Architecture and Design

DCGAN (Radford et al., 2015) adapts GANs to deep convolutional networks. The key design choices are:

- **Generator:** Uses transposed convolution (also known as deconvolution) layers to upsample a latent vector into an image. Batch normalization and ReLU activations (except for the output layer, which uses tanh) are applied.
- **Discriminator:** Employs convolutional layers to downsample the input image, with LeakyReLU activations and batch normalization (except the first layer).

These architectural choices lead to more stable training and higher-quality images.

### 2.2. Mathematical Operations

The generator transforms a noise vector $z \in \mathbb{R}^{d}$ into an image $G(z) \in \mathbb{R}^{H \times W \times C}$ through a series of learned deconvolutions:

$$
G(z) = \text{tanh}\Big(\text{ConvTranspose}_L(\cdots \text{ReLU}(\text{ConvTranspose}_1(z))\cdots)\Big).
$$

The discriminator computes a scalar probability $D(x)$ using convolutional layers:

$$
D(x) = \sigma\Big(\text{Conv}_L(\cdots \text{LeakyReLU}(\text{Conv}_1(x))\cdots)\Big),
$$

where $\sigma$ is the sigmoid function.

### 2.3. Pseudocode for DCGAN Training Loop

Below is simplified pseudocode capturing the essence of DCGAN training:

```python
# Pseudocode for DCGAN training

# Hyperparameters
batch_size = 64
lr = 0.0002
num_epochs = 100
latent_dim = 100

# Initialize generator G and discriminator D
G = Generator()   # Uses ConvTranspose layers, BatchNorm, ReLU, tanh at output
D = Discriminator()  # Uses Conv layers, BatchNorm, LeakyReLU, sigmoid at output

optimizer_G = Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))
optimizer_D = Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))

for epoch in range(num_epochs):
    for real_images in dataloader:   # real_images shape: (batch_size, C, H, W)
        # Train Discriminator
        z = sample_noise(batch_size, latent_dim)
        fake_images = G(z)
        
        # Compute loss for D (maximize log(D(x)) + log(1-D(G(z))))
        loss_D = - (log(D(real_images)) + log(1 - D(fake_images))).mean()
        
        optimizer_D.zero_grad()
        loss_D.backward()
        optimizer_D.step()

        # Train Generator: maximize log(D(G(z))) or minimize -log(D(G(z)))
        z = sample_noise(batch_size, latent_dim)
        fake_images = G(z)
        loss_G = - log(D(fake_images)).mean()
        
        optimizer_G.zero_grad()
        loss_G.backward()
        optimizer_G.step()
```

---

## 3. WGAN: Wasserstein GAN<a name="wgan"></a>

### 3.1. Motivation and Key Differences

Wasserstein GAN (WGAN) was proposed to address issues such as mode collapse and training instability observed with the original GAN formulation. Instead of minimizing the Jensen–Shannon divergence, WGAN minimizes the Earth Mover’s (Wasserstein-1) distance between the real and generated distributions.

### 3.2. The Wasserstein Distance

The Wasserstein distance $W(p_r, p_g)$ between two distributions $p_r$ (real) and $p_g$ (generated) is defined as:

$$
W(p_r, p_g) = \inf_{\gamma \in \Pi(p_r, p_g)} \mathbb{E}_{(x,y) \sim \gamma} \big[\|x - y\|\big],
$$

where $\Pi(p_r, p_g)$ is the set of all joint distributions with marginals $p_r$ and $p_g$. By the Kantorovich-Rubinstein duality, this can be rewritten as:

$$
W(p_r, p_g) = \sup_{\|f\|_L \le 1} \mathbb{E}_{x \sim p_r} [f(x)] - \mathbb{E}_{z \sim p_z} [f(G(z))],
$$

where $f$ is a 1-Lipschitz function. In WGAN, the discriminator (often renamed the “critic”) approximates $f$.

### 3.3. WGAN Loss and Weight Clipping

The critic’s loss is:

$$
L_{\text{critic}} = - E_{x \sim p_r} [f(x)] + E_{z \sim p_z} [f(G(z))],
$$

and the generator minimizes:

$$
L_{\text{gen}} = - \mathbb{E}_{z \sim p_z} [f(G(z))].
$$

To enforce the Lipschitz constraint ($\|f\|_L \le 1$), WGAN uses weight clipping: all weights of the critic are clipped to a small range (e.g., $[-0.01, 0.01]$).

### 3.4. Pseudocode for WGAN Training Loop

```python
# Pseudocode for WGAN training

# Hyperparameters
critic_iterations = 5
clip_value = 0.01
batch_size = 64
latent_dim = 100

G = Generator()   # As before
C = Critic()      # Similar to discriminator but without sigmoid

optimizer_G = RMSprop(G.parameters(), lr=0.00005)
optimizer_C = RMSprop(C.parameters(), lr=0.00005)

for epoch in range(num_epochs):
    for i, real_images in enumerate(dataloader):
        # Train critic more times than generator
        for _ in range(critic_iterations):
            z = sample_noise(batch_size, latent_dim)
            fake_images = G(z)
            
            loss_C = - (C(real_images).mean() - C(fake_images).mean())
            optimizer_C.zero_grad()
            loss_C.backward()
            optimizer_C.step()
            
            # Weight clipping for Lipschitz constraint
            for p in C.parameters():
                p.data.clamp_(-clip_value, clip_value)
        
        # Train generator
        z = sample_noise(batch_size, latent_dim)
        fake_images = G(z)
        loss_G = - C(fake_images).mean()
        
        optimizer_G.zero_grad()
        loss_G.backward()
        optimizer_G.step()
```

---

## 4. WGAN-GP: WGAN with Gradient Penalty<a name="wgangp"></a>

### 4.1. Improving WGAN with Gradient Penalty

WGAN’s weight clipping can lead to optimization challenges. WGAN-GP (WGAN with Gradient Penalty) replaces weight clipping with a gradient penalty that enforces the Lipschitz constraint more smoothly. Instead of forcing the weights to lie within a small interval, the gradient penalty term ensures that the gradient norm of the critic with respect to its input is close to 1.

### 4.2. Mathematical Formulation

The modified critic loss with gradient penalty is:

$$
L_{\text{critic}} = - \mathbb{E}_{x \sim p_r} [f(x)] + \mathbb{E}_{z \sim p_z} [f(G(z))] + \lambda \, \mathbb{E}_{\hat{x} \sim p_{\hat{x}}} \big[(\|\nabla_{\hat{x}} f(\hat{x})\|_2 - 1)^2\big],
$$

where:
- $\hat{x}$ is sampled uniformly along straight lines between pairs of real and generated samples.
- $\lambda$ is a penalty coefficient (commonly set to 10).

The generator’s loss remains:

$$
L_{\text{gen}} = - \mathbb{E}_{z \sim p_z} [f(G(z))].
$$

### 4.3. Pseudocode for WGAN-GP Training Loop

```python
# Pseudocode for WGAN-GP training

def gradient_penalty(C, real_data, fake_data, batch_size, lambda_gp):
    # Sample epsilon uniformly from [0, 1]
    epsilon = torch.rand(batch_size, 1, 1, 1, device=real_data.device)
    # Interpolate between real and fake data
    interpolates = epsilon * real_data + (1 - epsilon) * fake_data
    interpolates.requires_grad_(True)
    # Compute critic output on interpolates
    critic_interpolates = C(interpolates)
    # Compute gradients of critic output with respect to interpolates
    gradients = torch.autograd.grad(outputs=critic_interpolates, inputs=interpolates,
                                    grad_outputs=torch.ones_like(critic_interpolates),
                                    create_graph=True, retain_graph=True, only_inputs=True)[0]
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    # Compute penalty
    penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()
    return penalty

# Hyperparameters
critic_iterations = 5
lambda_gp = 10
batch_size = 64
latent_dim = 100

G = Generator()   # Same as before
C = Critic()      # Critic network without weight clipping

optimizer_G = Adam(G.parameters(), lr=0.0001, betas=(0.0, 0.9))
optimizer_C = Adam(C.parameters(), lr=0.0001, betas=(0.0, 0.9))

for epoch in range(num_epochs):
    for i, real_images in enumerate(dataloader):
        for _ in range(critic_iterations):
            z = sample_noise(batch_size, latent_dim)
            fake_images = G(z)
            
            loss_C = - (C(real_images).mean() - C(fake_images).mean())
            gp = gradient_penalty(C, real_images, fake_images, batch_size, lambda_gp)
            loss_C_total = loss_C + gp
            
            optimizer_C.zero_grad()
            loss_C_total.backward()
            optimizer_C.step()
        
        # Train generator
        z = sample_noise(batch_size, latent_dim)
        fake_images = G(z)
        loss_G = - C(fake_images).mean()
        
        optimizer_G.zero_grad()
        loss_G.backward()
        optimizer_G.step()
```

---

## 5. Summary, Key Intuitions and Implementation<a name="summary"></a>

- **DCGAN** leverages deep convolutional architectures to generate realistic images, with carefully chosen activation functions (ReLU, LeakyReLU, tanh) and batch normalization for stability.
- **WGAN** replaces the original GAN loss with the Wasserstein (Earth Mover’s) distance, which provides smoother gradients and improved training dynamics, enforced via weight clipping.
- **WGAN-GP** further refines WGAN by substituting weight clipping with a gradient penalty that directly enforces a 1-Lipschitz constraint, resulting in even more stable training.

Mathematically, these approaches change the way the divergence between the real and generated distributions is 
measured—from the Jensen–Shannon divergence (GAN) to the Wasserstein distance (WGAN)—and they modify the optimization 
landscape accordingly.

The provided pseudocode outlines training loops and key operations, offering a hands-on look at how these models are 
implemented. Together, these architectures have pushed the boundaries of generative modeling and continue to influence 
new research in machine learning. Here is a promised link to [my full implementation of DCGAN, WGAN, and WGAN-GP on CeleA dataset](https://gitlab.com/korzeniowski.renard/gan-optimization).

