# ICML 2024: Physics of LLMs a Star of the Show

The International Conference on Machine Learning (ICML) 2024 delivered a vibrant showcase of innovations ranging from fresh theoretical insights into language models to groundbreaking generative techniques in video and diffusion modeling. This year’s conference featured an exciting mix of tutorials and award-winning papers that not only push the state-of-the-art but also challenge our understanding of how models work and learn. In this post, we dive into some of the highlights, with a focus on the "Physics of Language Models" tutorial and three best paper awards.

<div style="text-align: center;">
  <img src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/icml-2024/icml-summary.png" alt="ICML Opening Remarks" style="width: 80%; max-width: 1000px; border-radius: 10px;">
</div>

_Fig. 1. ICML opening remarks slide summarizing most frequent key words used in the titles of accepted papers. ([Source Link](https://icml.cc/virtual/2024/remarks/37758))_

---

## Tutorials: Re-thinking the Physics of Language Models

One of the standout tutorials at ICML 2024, [Physics of Language Models](https://icml.cc/virtual/2024/tutorial/35223), offered deep insights into how large language models (LLMs) store and extract knowledge—and why their performance hinges on subtle aspects of training data and methodology. Here are some of the key points:

### Knowledge Storage, Extraction, and Data Quality
LLMs have the capacity to store vast amounts of world knowledge, but how that knowledge is embedded is crucial. Research shows that without **pretraining data rewriting (augmentation)**, the model may recite factual data verbatim while embedding that information diffusely across many tokens. This dispersion makes retrieval difficult when the model is later prompted. For example, simply including source tokens (such as domain names) can cue the model to identify high-quality, useful data. In contrast, training on “junk” data dramatically reduces the model’s effective capacity—even if the raw token count is high. In short, data quality and augmentation during pretraining are essential: addressing these issues later during fine-tuning is often too late to unlock the model’s true knowledge extraction capabilities.

### Error Measurement and Limitations
The tutorial also challenged us to think critically about how we quantify error—or “regret”—in language models. It turns out that a simplistic metric may hide critical limitations. Without proper data augmentation, even basic knowledge extraction tasks (such as retrieving a person’s birth date) can fail miserably. This isn’t simply a matter of model size or training duration; it’s about how the knowledge is encoded. The findings suggest that the model’s performance on hard tasks is dictated by what is learned during pretraining. Therefore, measuring “regret” must account not only for output accuracy but also for the underlying embedding quality, which, if not enhanced through augmentation, can lead to near-zero extraction accuracy even on simple queries.

### Unidirectional vs. Bidirectional Models
A particularly intriguing result from recent studies is that bidirectional models—like BERT—struggle to extract detailed factual knowledge after fine-tuning. In contrast, unidirectional models are far more effective. The reason is that in bidirectional architectures, cheat by looking ahead and never really learn more complex knowledge retrival techniques. While this gives them perfect scores on simplistic testsets, since all the public data was already used to train the model, when evaluate thoroughly on unseen data it exposes a complete inability to perform even the simplest tasks like retrieve a person’s birth month. At the same time a unidirectional model can do it without any issues. This insight encourages practitioners to think a bit deeper about seemingly insignificant details when designing models. Those details can have profound impact.

### Rethinking the “Dumb” Nature of Language Models
It’s a common misconception to label LLMs as “dumb” statistical machines that merely regurgitate learned token probabilities. In reality, their ability to store vast amounts of knowledge is remarkable—but only if that knowledge is encoded in a retrievable, structured way. Recent work has demonstrated that with proper mixed training and extensive data augmentation, LLMs can learn to correlate auxiliary cues (like diverse writing styles or source identifiers) with factual content. Without such augmentation, however, the stored knowledge may be so diffused that even if the model appears to recite facts accurately, it struggles to generalize and answer related questions. This nuanced view underscores that LLMs are not inherently “dumb” but are highly sensitive to the quality and structure of their pretraining data.

<div style="text-align: center;">
  <img src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/icml-2024/physics-of-llms.png" alt="ICML Opening Remarks" style="width: 80%; max-width: 1000px; border-radius: 10px;">
</div>

_Fig. 2. Physics of Language Models tutorial introduction slide. ([Source Link](https://icml.cc/virtual/2024/tutorial/35223))_

These updated insights from ICML 2024 remind us that the magic behind LLMs isn’t just in their architecture or scale—it’s in how we prepare and augment the training data, how we measure their performance, and even in the directionality of the model’s design. Addressing these factors early in pretraining is key to unlocking robust, flexible knowledge extraction and manipulation capabilities. There is a public version of this session shared by the author available in three parts: [Knowledge Storage and Extraction](https://arxiv.org/html/2309.14316v3), [Knowledge Manipulation](https://arxiv.org/html/2309.14402v2), [Knowledge Capacity Scaling Laws](https://arxiv.org/abs/2404.05405). I couldn't recommend enough putting aside an hour or two for a read.

---

## Best Papers: Innovations in Generative Modeling

### VideoPoet: A Large Language Model for Zero-Shot Video Generation

Awarded best paper, VideoPoet introduces a novel approach to video generation that leverages a large language model specifically tailored for the task. Its key contributions include:

<video style="float: right" width="30%" controls>
  <source src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/icml-2024/video_poet-video.mp4" type="video/mp4">
</video>

_Video. 1. Zeroshot example generated with VideoPoet only using text prompt "A bear with the head of an owl screeches loudly" as an input. ([Source Link](https://sites.research.google/videopoet/))_

- **Tokenized Video Generation:**  
  VideoPoet utilizes tokenized data from both text-paired and unpaired videos to train an LLM capable of generating video content in a zero-shot manner.

- **Video Super-Resolution in Latent Space:**  
  A standout feature is its method for increasing spatial resolution within the latent token space. This is achieved using a bidirectional transformer equipped with efficient windowed local attention, which enhances video quality and motion realism.

- **State-of-the-Art Performance:**  
  Evaluations demonstrate that VideoPoet not only competes with—but in many cases outperforms—current generative models, pushing the envelope in terms of realistic video generation.


![Image](https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/icml-2024/video_poet.png)

_Fig. 3. VideoPoet Overview: a versatile video generator that conditions on multiple types of inputs and performs a
variety of video generation tasks. ([Source Link](https://arxiv.org/pdf/2312.14125))_

However, the work also acknowledges limitations. The reliance on compressed and quantized tokens imposes an upper bound on visual fidelity, particularly for small objects and fine-grained details. Additionally, aesthetic biases in static scenes indicate that training data selection (e.g., excluding copyrighted content) can significantly impact performance.

### Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution

Another best paper award went to research that reimagines diffusion models for discrete spaces. Traditional diffusion models rely on score matching, but this approach has struggled to extend effectively to discrete data. The authors propose **score entropy**, a novel loss function that:

![Image](https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/icml-2024/discrete_diff.png)

_Fig. 4. Quality evaluation of unconditionally generated text with discrete diffusion model. ([Source Link](https://arxiv.org/pdf/2310.16834))_

- **Extends Score Matching to Discrete Domains:**  
  By incorporating score entropy, the method naturally adapts score matching for discrete data structures, integrating seamlessly into a discrete diffusion framework.

- **Competitive Performance:**  
  This discrete diffusion model not only outperforms existing language diffusion paradigms but also competes with autoregressive models such as GPT-2, marking a significant advancement in discrete generative modeling.

### Mixture-of-Experts in the Era of LLMs: A New Odyssey

The third best paper, titled *Mixture-of-Experts in the Era of LLMs: A New Odyssey*, explores how mixture-of-experts (MoE) architectures can be harnessed to scale large language models more efficiently. Although details are still emerging, the work promises to address challenges related to model scalability and computational efficiency while maintaining or even enhancing performance.

---

## Concluding Thoughts

ICML 2024 showcased a rich tapestry of ideas that stretch across generative modeling, language understanding, and architectural efficiency. The **Physics of Language Models** tutorial provided deep insights into how LLMs function and highlighted the importance of robust pretraining, data quality, and error measurement. Meanwhile, the best paper awards illustrate the cutting edge in generative research—from zero-shot video generation with VideoPoet and the extension of diffusion models to discrete spaces, to innovative approaches with MoE in LLMs.

Together, these contributions reflect a maturing field that is not only refining its mathematical and algorithmic foundations but is also beginning to answer fundamental questions about data quality, model scalability, and even the nature of learning itself.

