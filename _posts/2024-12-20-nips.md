# NeurIPS 2024: LMMs, Diffusion and Reasoning

The NeurIPS 2024 conference delivered another groundbreaking collection of research, reaffirming the field’s rapid evolution. From papers that rethink image generation to tutorials refining language model pipelines and invited talks drawing inspiration from childhood learning, this year’s conference showcased both rigorous mathematical innovation and bold new ideas. In this post, we highlight a few standout contributions and explore what they mean for the future of machine learning.

## Best Papers

### Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction

One of the most talked-about best paper awards went to a work on [Visual Autoregressive Modeling](https://openreview.net/pdf?id=gojL67CfS8). Instead of following the common approach of splitting images into patches, the authors propose a coarse-to-fine “next-resolution prediction” strategy. Key highlights include:
- **Multi-scale Prediction:** The model generates images by predicting the next resolution level rather than isolated patches.
- **Versatility in Image Editing:** It can handle in-painting, out-painting, and editing seamlessly.
- **Performance Gains:** According to the authors, this approach outperforms diffusion transformers in image quality, inference speed, data efficiency, and scalability—surpassing even Stable Diffusion 3.0 and SORA.
- **Scaling Laws & Zero-shot Capabilities:** Emulating properties observed in large language models, the system scales predictably and can generalize without explicit fine-tuning.
- **Architecture:** It combines a GPT-2-like transformer with a multi-scale VQVAE, and the models and code are open source.

![Image](https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nips-2024/visual-autoreg-modeling.png)

_Fig. 1. Standard autoregressive modeling vs. approach proposed by visual autoregressive modeling. ([Source Link](https://openreview.net/pdf?id=gojL67CfS8))_

This paper illustrates how rethinking the generation process at different scales can provide both mathematical elegance and practical benefits.

### Stochastic Taylor Derivative Estimator: Efficient Amortization for Arbitrary Differential Operators

Another intriguing award winner [Stochastic Taylor Derivative Estimator](https://openreview.net/pdf?id=J2wI2rCG2u) - presented a novel method for handling high-order differential operators in large-scale problems. Its core ideas include:
- **Stochastic Estimation:** The work introduces stochastic estimators for arbitrary differential operators.
- **Automatic Differentiation in Taylor Mode:** By leveraging AD in a Taylor expansion framework, the method efficiently amortizes the computation of high-order derivatives.
- **Large-Scale Applicability:** This approach could be a game changer in scenarios where evaluating complex differential operators is a computational bottleneck.

The mathematical depth here is striking—providing a robust tool for problems that traditionally required cumbersome numerical approximations.

### Not All Tokens Are What You Need for Pretraining

A third celebrated paper [Not All Tokens Are What You Need for Pretraining](https://openreview.net/pdf?id=0NMzBwqaAJ) - focused on selective language modeling. Instead of training on every token indiscriminately, this work proposes:
- **Selective Training:** The language model is trained only on tokens that align with a desired target distribution.
- **Token Scoring:** A reference model scores tokens, allowing the main model to focus on “useful” tokens.
- **Focused Loss Function:** The loss is computed selectively, which leads to more efficient and effective pretraining.

![Image](https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nips-2024/train-token-selection.png)

_Fig. 2. Left: Causal Language Modeling that trains on all tokens. Right: Selective
Language Modeling approach selectively applies loss on those useful and clean tokens. ([Source Link](https://openreview.net/pdf?id=0NMzBwqaAJ))_

This work addresses inefficiencies in conventional language model training and provides new insights into the importance of data selection—a concept with both mathematical and practical implications.

### Guiding a Diffusion Model with a Bad Version of Itself

Finally, another standout paper [Guiding a Diffusion Model with a Bad Version of Itself](https://openreview.net/pdf?id=bg6fVPVs3s) - introduced a method to guide diffusion models using a “bad” (i.e., smaller, less-trained) version of the same model:
- **Disentangled Control:** This technique allows for fine control over image quality while preserving diversity.
- **Avoiding CFG Pitfalls:** Unlike traditional classifier-free guidance, which can suppress variability at high noise levels, this method maintains a desirable balance.
- **Practical Impact:** The approach offers a promising avenue for achieving both high fidelity and variety in generated images.

![Image](https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nips-2024/auto-guidance.png)

_Fig. 3. A fractal-like 2D distribution with two classes indicated with gray and orange region. Figure ilustrates how well points sampled with different diffusion approaches concentrate on high-probability regions and what's the level of diversity. ([Source Link](https://openreview.net/pdf?id=bg6fVPVs3s))_

Each of these papers uses rigorous mathematical frameworks to tackle long-standing issues—from stability and efficiency to data selection and controllability.

### Bridging the Gap Between Compression and Generation

One notable contribution in the realm of neural audio is [A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation](https://arxiv.org/abs/2410.22448). Key points include:
- **Resynthesis from Coarse Tokens:** The paper examines how to better reconstruct waveforms from compressed tokens without additional task-specific information.
- **Target Choices Matter:** It was shown that regressing to a continuous (pre-quantized) embedding yields superior audio quality compared to discrete token prediction.
- **Iterative Methods:** Iterative refinement techniques not only improve sound quality but also help mitigate overfitting.
- **Schrödinger Bridges Connection:** The work even draws connections from diffusion models to flow models and Schrödinger Bridges, underscoring deep mathematical links across generative approaches.

These findings illustrate that careful choices in learning targets and resynthesis strategies can dramatically affect the end quality—a reminder that good objective scores do not always translate into perceptual quality.

<div style="text-align: center;">
  <img src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nips-2024/codec-nips2024.png" alt="ICML Opening Remarks" style="width: 80%; max-width: 1000px; border-radius: 10px;">
</div>

_Fig. 3. On the left illustration of neural codec compression with three codebooks. RVQ stands for Residual Vector Quantization. On the right codec resynthesis methods that generates audio from the first RVQ code. ([Source Link](https://arxiv.org/pdf/2410.22448))_


## Tutorials

NeurIPS 2024 also featured tutorials that offered hands-on insights into state-of-the-art practices:

### [Opening the Language Model Pipeline](https://neurips.cc/virtual/2024/tutorial/99526)
This tutorial covered everything from data preparation to model training and adaptation. Some key takeaways:
- **Linearization:** Transform raw data into valid text formats.
- **Optimization Techniques:** Use activation functions like swish, GLU for long-range dependencies, and optimizers such as AdamW with RMSNorm and cosine learning rate schedulers.
- **Stability Fixes:** Emphasized ensuring that activations and gradients remain consistent across layers—crucial for scaling model width.
- **Efficiency Considerations:** Both pretraining experiments on smaller models and curriculum learning for new data were discussed, along with hints on hardware efficiency improvements like FlashAttn.

### [Watermarking for Large Language Models](https://neurips.cc/virtual/2024/tutorial/99521)
This session delved into embedding robust watermarks into LLM outputs:
- **Dimensions of Watermarking:** Balancing quality, detectability (minimizing type I/II errors), robustness, and security.
- **Attack Vectors:** Discussed evasion and spoofing attacks, and introduced a “Red Green” watermarking strategy that tweaks token probabilities.
  
### [Causality for Large Language Models](https://neurips.cc/virtual/2024/tutorial/99520)
Addressing causal reasoning, this tutorial explored:
- **Causal Graph Discovery:** Leveraging LLMs to identify variable relationships and estimate causal effects.
- **Ladder of Causation:** Emphasized moving from mere association to interventions and counterfactual reasoning.
- **Interpretability:** How causal methods can provide deeper insights into LLM behavior, connecting theoretical work (e.g., Pearl’s causality framework) with practical model design.

## Invited Talks

Beyond papers and tutorials, NeurIPS 2024 featured compelling invited talks that bridged diverse disciplines.

### [The Golem vs. Stone Soup: Understanding How Children Learn Can Help Us Improve AI](https://neurips.cc/virtual/2024/invited-talk/101128)
This talk proposed that the way children learn—characterized by an exploratory, exploitative phase—can inform AI design:
- **Childhood as Exploration:** Highlighted that early learning is about exploring a vast hypothesis space, analogous to current practices in model pretraining.
- **Caregiver Roles:** Suggested that AI could be made more “caring” by maximizing recipient utilities and empowerment, drawing on insights from social contracts and caregiving in human society.

### [From Diffusion Models to Schrödinger Bridges](https://neurips.cc/virtual/2024/invited-talk/101133)
A detailed invited talk traced the evolution from diffusion models to more general flow models and finally to Schrödinger Bridges:
- **Mathematical Pathways:** Explored how iterative refinements in diffusion processes can be mathematically connected to optimal transport and flow formulations.
- **Practical Implications:** These insights are not only of theoretical interest but also guide new methods for controlling generative models.

## Final Thoughts

NeurIPS 2024 demonstrated that the frontier of machine learning is as vibrant as ever. Researchers are rethinking generation—from visual and audio domains to language—by refining the underlying mathematics and challenging existing paradigms. Whether through novel loss formulations in GANs, selective training in language models, or innovative strategies in neural codec resynthesis, the conference underscored that blending theory with practical engineering leads to substantial leaps forward.

For practitioners, the tutorials provided actionable strategies for improving model training, stability, and efficiency. For researchers, the best paper awards and invited talks opened new directions—highlighting the importance of mathematical rigor, efficient computation, and even insights drawn from human cognition.

As NeurIPS 2024 unfolds, one thing is clear: our understanding of generative models, differential estimation, and language modeling is rapidly evolving, paving the way for the next generation of intelligent systems.
