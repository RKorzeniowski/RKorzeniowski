# Neural Radiance Fields and Cool Things You Can Do With Them

Neural Radiance Fields (NeRF) have rapidly emerged as one of the most exciting representations for 3D scenes using deep learning. Originally introduced by Mildenhall et al. in 2020, NeRF uses a continuous function—realized as a multilayer perceptron (MLP)—to encode a scene. The network takes in spatial coordinates and viewing directions and outputs color and density, which are then used in a differentiable volume rendering process to synthesize photorealistic novel views.

In this post, we will break down NeRF’s inner workings from a mathematical perspective, illustrate the role of positional encoding, provide pseudocode for clarity, and share intuitive explanations along the way.

---

## 1. Overview and Intuition

At its heart, NeRF represents a scene as a function  

$$
F_\theta: \mathbb{R}^3 \times \mathbb{S}^2 \to \mathbb{R}^4,
$$  

where the input is a 3D coordinate $\mathbf{x} = (x,y,z)$ and a 2D viewing direction $\mathbf{d}$ (typically parameterized as $(\theta, \phi)$ ). The output is a 4D vector $(r, g, b, \sigma)$ where $(r, g, b)$ are the emitted color values and $\sigma$ is the volume density.

### Why It Works

NeRF’s magic comes from the idea of “learning” the scene’s radiance field. Rather than storing explicit geometry (like a mesh or voxel grid), NeRF encodes all the visual details into the weights of a neural network. When queried with a coordinate and viewing direction, the network predicts what color would be seen and how much light would be absorbed (density) at that point. This approach allows for:
- **Continuous Representations:** The function $F_\theta$ is continuous, meaning it can be evaluated at arbitrarily high resolutions.
- **View-Dependence:** By taking the viewing direction as an input, NeRF can model effects such as specular highlights.
- **Differentiability:** The rendering process is fully differentiable, which lets you train the network end-to-end using standard gradient descent.

For an intuitive picture, imagine “compressing” a 3D scene into a tiny MLP. Later, by “probing” this network along a ray that passes through the scene, you can recover both the color and the density at every point. Then, by integrating these along the ray, you generate the final pixel color.

---

## 2. Mathematical Formulation

### 2.1. The NeRF Function

The neural network $F_\theta$ is designed to approximate the mapping:

$$
F_\theta(\mathbf{x}, \mathbf{d}) = (c, \sigma)
$$

where:
- $\mathbf{x} \in \mathbb{R}^3$ is the 3D position.
- $\mathbf{d} \in \mathbb{S}^2$ is the unit vector for the viewing direction.
- $c = (r, g, b) \in [0, 1]^3$ is the RGB color.
- $\sigma \in \mathbb{R}_{\ge 0}$ is the volume density.

Because the network is a universal function approximator, it is able to learn both the scene’s geometry and its view-dependent appearance.

### 2.2. Positional Encoding

Standard MLPs struggle to learn high-frequency functions, which is critical for rendering fine details. To overcome this, NeRF applies a positional encoding $\gamma(\cdot)$ to the inputs before passing them to the network. A common encoding is the Fourier feature mapping:

$$
\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \dots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p) \right)
$$

for each component of $\mathbf{x}$ (and optionally for $\mathbf{d}$ ). This mapping lifts the input into a higher-dimensional space, making it easier for the network to learn high-frequency variations.

### 2.3. Volume Rendering Equation

Once the network outputs density and color at a sampled point, the next step is to compute the color of a pixel by integrating these values along a camera ray. Let a ray be defined as:

$$
\mathbf{r}(t) = \mathbf{o} + t \mathbf{d}, \quad t \in [t_n, t_f]
$$

where $\mathbf{o}$ is the ray origin and $\mathbf{d}$ is its direction. The final pixel color $C(\mathbf{r})$ is computed using:

$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\, \sigma(\mathbf{r}(t))\, c(\mathbf{r}(t), \mathbf{d}) \, dt,
$$

with the transmittance defined as:

$$
T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s))\, ds\right).
$$

This formulation essentially weights the contribution of each sample along the ray by how much light “survives” to reach that point.

In practice, the integral is approximated via quadrature by sampling a fixed number $N$ of points along the ray.

---

## 3. The NeRF Training and Inference Pipeline

### 3.1. Training Pipeline

1. **Data Acquisition:**  
   Collect a set of images of the scene along with corresponding camera poses (using, for example, COLMAP).

2. **Sampling Points Along Rays:**  
   For each pixel in an image, generate a ray. Then sample $N$ points along this ray uniformly (or using hierarchical sampling for efficiency).

3. **NeRF Inference:**  
   For each sampled point, apply positional encoding and feed it (along with the viewing direction) into the MLP $F_\theta$ to obtain $(c_i, \sigma_i)$.

4. **Volume Rendering:**  
   Approximate the pixel color using a discrete sum:

   $\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i \left(1 - \exp(-\sigma_i \delta_i)\right) c_i,$

   where 

   $T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j \right),$

   and $\delta_i$ is the distance between adjacent sample points.

6. **Loss Computation:**  
   Compute the reconstruction loss (e.g., mean squared error) between the rendered pixel colors $\hat{C}(\mathbf{r})$ and the ground-truth image.

7. **Backpropagation:**  
   Use gradient descent to update the network parameters $\theta$.

### 3.2. Inference Pipeline

At inference time, the trained NeRF is used to render novel views by:
- Placing a virtual camera in the scene.
- Generating rays for each pixel.
- Sampling points along each ray.
- Querying the network $F_\theta$ and compositing the outputs using the volume rendering equation.

---

## 4. Pseudocode for NeRF Training

Below is a simplified pseudocode that captures the main training loop for a NeRF model:

```python
# Pseudocode for NeRF Training

# Inputs:
# images: List of training images
# poses: Corresponding camera poses
# N: Number of samples per ray
# L: Number of frequency bands for positional encoding
# F_theta: MLP network with parameters theta

for epoch in range(num_epochs):
    for image, pose in dataset:
        for ray in generate_rays(image, pose):
            # Sample points along the ray
            t_vals = linspace(t_near, t_far, N)
            points = [ray.origin + t * ray.direction for t in t_vals]
            
            # Apply positional encoding to each point
            encoded_points = [positional_encoding(p, L) for p in points]
            encoded_dir = positional_encoding(ray.direction, L_dir)
            
            # Query the network for each encoded point
            outputs = [F_theta(encoded_p, encoded_dir) for encoded_p in encoded_points]
            colors, densities = unzip(outputs)  # Each output is (c, sigma)
            
            # Compute accumulated color along the ray using volume rendering
            T = 1.0
            pixel_color = 0.0
            for i in range(N):
                delta = t_vals[i+1] - t_vals[i] if i < N - 1 else t_vals[-1] - t_vals[-2]
                alpha = 1 - exp(-densities[i] * delta)
                weight = T * alpha
                pixel_color += weight * colors[i]
                T *= exp(-densities[i] * delta)
            
            # Compute loss between rendered pixel_color and ground truth
            loss += mse(pixel_color, ground_truth_pixel(ray))
    
    # Update network parameters
    theta = optimizer.step(loss)
```

This pseudocode outlines the core operations:
- **Ray sampling and positional encoding:** Every point on a ray is lifted to a higher-dimensional space.
- **Network evaluation:** The MLP predicts color and density for each point.
- **Volume rendering:** The contributions from each point are composited along the ray.
- **Loss and optimization:** The network parameters are updated to minimize the error between rendered and real images.
---

## 5. Intuitions Behind the Mathematics

### Why Positional Encoding Matters

Neural networks tend to have a spectral bias toward learning low-frequency functions. This means that if you feed raw coordinates directly, the network may only capture coarse, blurry details. Positional encoding transforms each coordinate into a set of high-frequency components, allowing the network to represent finer details. Think of it as giving the network “magnifying glasses” at multiple scales.

### Volume Rendering as Ray Integration

Volume rendering integrates the contributions of color and density along each ray. The transmittance $T(t)$ can be thought of as the “remaining light” that isn’t absorbed by the scene up to that point. This is analogous to how light gets progressively blocked by semi-transparent objects. The final pixel value is a weighted sum of colors where areas with high density (i.e., surfaces) contribute more significantly.

### Continuous vs. Discrete Representations

Unlike traditional methods that require explicit 3D geometry (meshes or voxel grids), NeRF represents scenes as continuous functions. This allows for smooth interpolation between views and the ability to render images at any resolution—all encoded in the network weights.

---

## 6. Applications and Future Directions

NeRF’s capability to generate photorealistic novel views has spurred a myriad of applications:
- **Virtual and Augmented Reality:** Generate immersive experiences by reconstructing real-world scenes.
- **3D Content Creation:** Transform image collections into 3D models for films, games, or digital twins.
- **Robotics and Autonomous Driving:** Create dense scene representations to aid in navigation and perception.
- **Medical Imaging:** Use NeRF-like models for reconstructing 3D anatomical structures from 2D scans.

Recent innovations such as Instant NeRFs—with techniques like multiresolution hash encoding—further speed up training and inference, bringing real-time applications closer to reality.

---

## 7. Conclusion

NeRF represents a paradigm shift in how we think about 3D scene representation and rendering. By combining the power of neural networks with classical volume rendering, NeRF can generate stunning novel views from just a few input images. The mathematical foundation—centered around positional encoding, volumetric integration, and differentiable rendering—underpins its success. As research continues to address challenges like training speed, dynamic scene handling, and editing flexibility, NeRF and its variants are poised to revolutionize 3D graphics, virtual reality, and beyond.
