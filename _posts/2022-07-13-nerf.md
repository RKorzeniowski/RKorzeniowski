# Neural Radiance Fields and Cool Things You Can Do With Them

Neural Radiance Fields (NeRF) have rapidly emerged as one of the most exciting representations for 3D scenes using deep learning. Originally introduced by Mildenhall et al. in 2020, NeRF uses a continuous function—realized as a multilayer perceptron (MLP)—to encode a scene. The network takes in spatial coordinates and viewing directions and outputs color and density, which are then used in a differentiable volume rendering process to synthesize photorealistic novel views.

In this post, we will break down NeRF’s inner workings from a mathematical perspective, illustrate the role of positional encoding, provide pseudocode for clarity, and share intuitive explanations along the way.


<video width="80%" controls>
   <source src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nerf/3d_virtual_boat.mp4" type="video/mp4">
</video>

_Video. 1. 3d scan of a small boat created with NeRF from a short phone recording of the object._

---

## 1. Overview and Intuition

At its heart, NeRF represents a scene as a function  

$$
F_\theta: \mathbb{R}^3 \times \mathbb{S}^2 \to \mathbb{R}^4,
$$  

where the input is a 3D coordinate $$\mathbf{x} = (x,y,z)$$ and a 2D viewing direction \( \mathbf{d} \) (typically parameterized as $(\theta, \phi)$ ). The output is a 4D vector $(r, g, b, \sigma)$ where $(r, g, b)$ are the emitted color values and $\sigma$ is the volume density.

### Why It Works

NeRF’s magic comes from the idea of “learning” the scene’s radiance field. Rather than storing explicit geometry (like a mesh or voxel grid), NeRF encodes all the visual details into the weights of a neural network. When queried with a coordinate and viewing direction, the network predicts what color would be seen and how much light would be absorbed (density) at that point. This approach allows for:
- **Continuous Representations:** The function $F_\theta$ is continuous, meaning it can be evaluated at arbitrarily high resolutions.
- **View-Dependence:** By taking the viewing direction as an input, NeRF can model effects such as specular highlights.
- **Differentiability:** The rendering process is fully differentiable, which lets you train the network end-to-end using standard gradient descent.

For an intuitive picture, imagine “compressing” a 3D scene into a tiny MLP. Later, by “probing” this network along a ray that passes through the scene, you can recover both the color and the density at every point. Then, by integrating these along the ray, you generate the final pixel color.

<video width="100%" controls>
   <source src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nerf/virtual_room_exploration.mp4" type="video/mp4">
</video>

_Video. 2. Short exploration of virtual reconstruction of my room created with NeRF._

---

## 2. Mathematical Formulation

### 2.1. The NeRF Function

The neural network $F_\theta$ is designed to approximate the mapping:

$$
F_\theta(\mathbf{x}, \mathbf{d}) = (c, \sigma)
$$

where:
- $\mathbf{x} \in \mathbb{R}^3$ is the 3D position.
- $\mathbf{d} \in \mathbb{S}^2$ is the unit vector for the viewing direction.
- $c = (r, g, b) \in [0, 1]^3$ is the RGB color.
- $\sigma \in \mathbb{R}_{\ge 0}$ is the volume density.

Because the network is a universal function approximator, it is able to learn both the scene’s geometry and its view-dependent appearance.

### 2.2. Positional Encoding

Standard MLPs struggle to learn high-frequency functions, which is critical for rendering fine details. To overcome this, NeRF applies a positional encoding $\gamma(\cdot)$ to the inputs before passing them to the network. A common encoding is the Fourier feature mapping:

$$
\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \dots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p) \right)
$$

for each component of $\mathbf{x}$ (and optionally for $\mathbf{d}$ ). This mapping lifts the input into a higher-dimensional space, making it easier for the network to learn high-frequency variations.

### 2.3. Volume Rendering Equation

Once the network outputs density and color at a sampled point, the next step is to compute the color of a pixel by integrating these values along a camera ray. Let a ray be defined as:

$$
\mathbf{r}(t) = \mathbf{o} + t \mathbf{d}, \quad t \in [t_n, t_f]
$$

where $\mathbf{o}$ is the ray origin and $\mathbf{d}$ is its direction. The final pixel color $C(\mathbf{r})$ is computed using:

$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\, \sigma(\mathbf{r}(t))\, c(\mathbf{r}(t), \mathbf{d}) \, dt,
$$

with the transmittance defined as:

$$
T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s))\, ds\right).
$$

This formulation essentially weights the contribution of each sample along the ray by how much light “survives” to reach that point.

In practice, the integral is approximated via quadrature by sampling a fixed number $N$ of points along the ray.

---

## 3. The NeRF Training and Inference Pipeline

### 3.1. Training Pipeline

1. **Data Acquisition:**  
   Collect a set of images of the scene along with corresponding camera poses (using, for example, COLMAP).

2. **Sampling Points Along Rays:**  
   For each pixel in an image, generate a ray. Then sample $N$ points along this ray uniformly (or using hierarchical sampling for efficiency).

3. **NeRF Inference:**  
   For each sampled point, apply positional encoding and feed it (along with the viewing direction) into the MLP $F_\theta$ to obtain $(c_i, \sigma_i)$.

4. **Volume Rendering:**  
   Approximate the pixel color using a discrete sum:

   $\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i \left(1 - \exp(-\sigma_i \delta_i)\right) c_i,$

   where 

   $T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j \right),$

   and $\delta_i$ is the distance between adjacent sample points.

6. **Loss Computation:**  
   Compute the reconstruction loss (e.g., mean squared error) between the rendered pixel colors $\hat{C}(\mathbf{r})$ and the ground-truth image.

7. **Backpropagation:**  
   Use gradient descent to update the network parameters $\theta$.

### 3.2. Inference Pipeline

At inference time, the trained NeRF is used to render novel views by:
- Placing a virtual camera in the scene.
- Generating rays for each pixel.
- Sampling points along each ray.
- Querying the network $F_\theta$ and compositing the outputs using the volume rendering equation.

---

## 4. Pseudocode for NeRF Training

Below is a simplified pseudocode that captures the main training loop for a NeRF model:

```python
# Pseudocode for NeRF Training

# Inputs:
# images: List of training images
# poses: Corresponding camera poses
# N: Number of samples per ray
# L: Number of frequency bands for positional encoding
# F_theta: MLP network with parameters theta

for epoch in range(num_epochs):
    for image, pose in dataset:
        for ray in generate_rays(image, pose):
            # Sample points along the ray
            t_vals = linspace(t_near, t_far, N)
            points = [ray.origin + t * ray.direction for t in t_vals]
            
            # Apply positional encoding to each point
            encoded_points = [positional_encoding(p, L) for p in points]
            encoded_dir = positional_encoding(ray.direction, L_dir)
            
            # Query the network for each encoded point
            outputs = [F_theta(encoded_p, encoded_dir) for encoded_p in encoded_points]
            colors, densities = unzip(outputs)  # Each output is (c, sigma)
            
            # Compute accumulated color along the ray using volume rendering
            T = 1.0
            pixel_color = 0.0
            for i in range(N):
                delta = t_vals[i+1] - t_vals[i] if i < N - 1 else t_vals[-1] - t_vals[-2]
                alpha = 1 - exp(-densities[i] * delta)
                weight = T * alpha
                pixel_color += weight * colors[i]
                T *= exp(-densities[i] * delta)
            
            # Compute loss between rendered pixel_color and ground truth
            loss += mse(pixel_color, ground_truth_pixel(ray))
    
    # Update network parameters
    theta = optimizer.step(loss)
```

This pseudocode outlines the core operations:
- **Ray sampling and positional encoding:** Every point on a ray is lifted to a higher-dimensional space.
- **Network evaluation:** The MLP predicts color and density for each point.
- **Volume rendering:** The contributions from each point are composited along the ray.
- **Loss and optimization:** The network parameters are updated to minimize the error between rendered and real images.
---

## 5. Intuitions Behind the Mathematics

### Why Positional Encoding Matters

Neural networks tend to have a spectral bias toward learning low-frequency functions. This means that if you feed raw coordinates directly, the network may only capture coarse, blurry details. Positional encoding transforms each coordinate into a set of high-frequency components, allowing the network to represent finer details. Think of it as giving the network “magnifying glasses” at multiple scales.

### Volume Rendering as Ray Integration

Volume rendering integrates the contributions of color and density along each ray. The transmittance $T(t)$ can be thought of as the “remaining light” that isn’t absorbed by the scene up to that point. This is analogous to how light gets progressively blocked by semi-transparent objects. The final pixel value is a weighted sum of colors where areas with high density (i.e., surfaces) contribute more significantly.

### Continuous vs. Discrete Representations

Unlike traditional methods that require explicit 3D geometry (meshes or voxel grids), NeRF represents scenes as continuous functions. This allows for smooth interpolation between views and the ability to render images at any resolution—all encoded in the network weights.

<div style="text-align: center;">
  <img src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski.github.io/refs/heads/main/images/nerf/3d_unity_doll.jpg" alt="scan unity" style="width: 80%; max-width: 1000px; border-radius: 10px;">
</div>

_Fig. 1. Wooden mannequin scan as an asset replicated six times inside Unity engine._

---

## 6. Cool Applications

I have spent a considerable amount of time experimenting with NeRFs. Although setting up all the required software was somewhat laborious, the results have been well worth the effort. I even set up a server that allowed you to send videos recorded with your phone and receive complete 3D models in return. Throughout the project, I accomplished three major tasks:

<video width="100%" controls>
   <source src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski/refs/heads/main/images/nerf/virtual_spin.mp4" type="video/mp4">
</video>

_Video. 3. Virtual representation of a wooden mannequin: in the original scene; only object removed from the original scene; only wooden mannequin and no background; density representation; disparity map._

- Game Asset Integration: I uploaded scans into the Unity engine, transforming them into usable assets for game development.
- 3D Printing: I used a 3D printer to create a physical replica of an existing object. Although some manual processing was required to make the models compatible with 3D printing software, this step could largely be automated.
- Virtual Room Reconstruction: I created a detailed virtual copy of my room, showcasing the potential for immersive digital experiences.

Moreover, numerous new projects are emerging that focus not only on reconstructing entire scenes but also on dedicated applications such as complex object reconstruction and virtual house tours. I am excited to explore these advancements and see how they further enhance the capabilities of NeRF technology.

<div style="text-align: center;">
  <img src="https://raw.githubusercontent.com/RKorzeniowski/rkorzeniowski.github.io/refs/heads/main/images/nerf/3d_original_printed_comparison.jpg" alt="Comparison" style="width: 80%; max-width: 1000px; border-radius: 10px;">
</div>

_Fig. 2. Original wooden mannequin next to a 3d printed small scale replica created with NeRF._

---

## 7. Conclusion

NeRF represents a paradigm shift in how we think about 3D scene representation and rendering. By combining the power of neural networks with classical volume rendering, NeRF can generate stunning novel views from just a few input images. The mathematical foundation—centered around positional encoding, volumetric integration, and differentiable rendering—underpins its success. As research continues to address challenges like training speed, dynamic scene handling, and editing flexibility, NeRF and its variants are poised to revolutionize 3D graphics, virtual reality.
